{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1: Classification and Vector Spaces\n",
    "# Week 1: Logistic Regression for Sentiment Analysis of Tweets\n",
    "## Assignment: Logistic Regression\n",
    "\n",
    "\n",
    "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\n",
    "\n",
    "- Learn how to extract features for logistic regression given some text\n",
    "- Implement logistic regression from scratch\n",
    "- Apply logistic regression on a natural language processing task\n",
    "- Test using your logistic regression\n",
    "- Perform error analysis\n",
    "We will be using a data set of tweets. Hopefully you will get more than 99% accuracy.\n",
    "Run the cell below to load in the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run lecture2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tqdm for notebooks\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The twitter_samples contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n",
    "- If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n",
    "- You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split both sets into training and testing sets\n",
    "# 80% training, 20% testing\n",
    "\n",
    "pos_train = pos_tweets[:int(0.8*len(pos_tweets))]\n",
    "neg_train = neg_tweets[:int(0.8*len(neg_tweets))]\n",
    "pos_test = pos_tweets[int(0.8*len(pos_tweets)):]\n",
    "neg_test = neg_tweets[int(0.8*len(neg_tweets)):]\n",
    "\n",
    "train_x = pos_train + neg_train\n",
    "test_x = pos_test + neg_test\n",
    "\n",
    "# Create the label numpy array\n",
    "train_y = np.append(np.ones(len(pos_train)), np.zeros(len(neg_train)))\n",
    "test_y = np.append(np.ones(len(pos_test)), np.zeros(len(neg_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8000 tweets\n",
      "Testing set: 2000 tweets\n",
      "Training set: <class 'list'>\n",
      "Testing set: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Print lengths and shapes\n",
    "print('Training set:', len(train_x), 'tweets')\n",
    "print('Testing set:', len(test_x), 'tweets')\n",
    "print('Training set:', type(train_x))\n",
    "print('Testing set:', type(test_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8430\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Build freqs for positive tweets\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "print(len(freqs))\n",
    "print(type(freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - z: input (scalar or array)\n",
    "    Output:\n",
    "    - sigmoid(z): sigmoid of z\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "### Logistic regression: cost and gradient\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions: Implement gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters, cost_history):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in tqdm(range(0, num_iters)):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
    "        print(theta)\n",
    "        cost_history.append(J)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J, theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  3.24869073e+03 -1.22351283e+03]\n",
      " [ 1.00000000e+00 -1.05634350e+03 -2.14593724e+03]\n",
      " [ 1.00000000e+00  1.73081526e+03 -4.60307739e+03]\n",
      " [ 1.00000000e+00  3.48962353e+03 -1.52241380e+03]\n",
      " [ 1.00000000e+00  6.38078192e+02 -4.98740751e+02]\n",
      " [ 1.00000000e+00  2.92421587e+03 -4.12028142e+03]\n",
      " [ 1.00000000e+00 -6.44834408e+02 -7.68108709e+02]\n",
      " [ 1.00000000e+00  2.26753888e+03 -2.19978253e+03]\n",
      " [ 1.00000000e+00 -3.44856415e+02 -1.75571684e+03]\n",
      " [ 1.00000000e+00  8.44274934e+01  1.16563043e+03]]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tmp_x = np.append(np.ones((10, 1)), np.random.randn(10, 2) * 2000, axis=1)\n",
    "tmp_y = (np.random.randn(10, 1) > 0.35).astype(float)\n",
    "\n",
    "print(tmp_x)\n",
    "print(tmp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fbc9eee108>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "cost_history = []\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700, cost_history)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")\n",
    "\n",
    "cost_history = np.array(cost_history).flatten()\n",
    "plt.plot(cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the features from the tweets\n",
    "Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "- The first feature is the number of positive words in a tweet.\n",
    "- The second feature is the number of negative words in a tweet.\n",
    "Then train your logistic regression classifier on these features.\n",
    "Test the classifier on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(tweet, freqs):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - freqs: a dictionary of frequencies of the type\n",
    "        {\n",
    "            \"word\": {\n",
    "                \"positive\": int,\n",
    "                \"negative\": int\n",
    "            }\n",
    "        }\n",
    "    Output:\n",
    "        - feature_vector: a numpy array of features of dimension (1,3)\n",
    "    '''\n",
    "    tokens = process_text(tweet)\n",
    "    x = np.zeros((1, 3))\n",
    "\n",
    "    x[0, 0] = 1\n",
    "    for token in tokens:\n",
    "        if token in freqs:\n",
    "            x[0, 1] += freqs[token]['positive']\n",
    "            x[0, 2] += freqs[token]['negative']\n",
    "\n",
    "    assert x.shape == (1, 3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.109e+03 6.100e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Check the feature vector for a tweet\n",
    "tmp1 = extract_feature(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "To train the model:\n",
    "\n",
    "- Stack the features for all training examples into a matrix X.\n",
    "- Call gradientDescent, which you've implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e7198b486d4a64916ab87383d7e431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in tqdm(range(len(test_x))):\n",
    "    X[i, :] = extract_feature(train_x[i], freqs)\n",
    "\n",
    "Y = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_feature(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.109e+03 6.100e+01]\n",
      " [1.000e+00 3.705e+03 4.440e+02]\n",
      " [1.000e+00 3.119e+03 1.160e+02]\n",
      " ...\n",
      " [1.000e+00 1.440e+02 7.920e+02]\n",
      " [1.000e+00 2.070e+02 3.901e+03]\n",
      " [1.000e+00 1.870e+02 3.985e+03]]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a137377c97f4793aa7286e42e1caaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00000000e-10  5.00000000e-10  5.00000000e-10 ... -5.00000000e-10\n",
      "  -5.00000000e-10 -5.00000000e-10]\n",
      " [ 7.24311500e-07  7.24311500e-07  7.24311500e-07 ... -7.24311500e-07\n",
      "  -7.24311500e-07 -7.24311500e-07]\n",
      " [ 9.77974188e-07  9.77974188e-07  9.77974188e-07 ... -9.77974188e-07\n",
      "  -9.77974188e-07 -9.77974188e-07]]\n",
      "[[ 9.99259471e-10  9.99259471e-10  9.99259471e-10 ... -9.99259471e-10\n",
      "  -9.99259471e-10 -9.99259471e-10]\n",
      " [ 1.44757364e-06  1.44757364e-06  1.44757364e-06 ... -1.44757364e-06\n",
      "  -1.44757364e-06 -1.44757364e-06]\n",
      " [ 1.95384754e-06  1.95384754e-06  1.95384754e-06 ... -1.95384754e-06\n",
      "  -1.95384754e-06 -1.95384754e-06]]\n",
      "[[ 1.49777983e-09  1.49777983e-09  1.49777983e-09 ... -1.49777983e-09\n",
      "  -1.49777983e-09 -1.49777983e-09]\n",
      " [ 2.16978808e-06  2.16978808e-06  2.16978808e-06 ... -2.16978808e-06\n",
      "  -2.16978808e-06 -2.16978808e-06]\n",
      " [ 2.92762452e-06  2.92762452e-06  2.92762452e-06 ... -2.92762452e-06\n",
      "  -2.92762452e-06 -2.92762452e-06]]\n",
      "[[ 1.99556248e-09  1.99556248e-09  1.99556248e-09 ... -1.99556248e-09\n",
      "  -1.99556248e-09 -1.99556248e-09]\n",
      " [ 2.89095650e-06  2.89095650e-06  2.89095650e-06 ... -2.89095650e-06\n",
      "  -2.89095650e-06 -2.89095650e-06]\n",
      " [ 3.89930960e-06  3.89930960e-06  3.89930960e-06 ... -3.89930960e-06\n",
      "  -3.89930960e-06 -3.89930960e-06]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13372\\2908417011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Apply gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradientDescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The cost after training is {J:.8f}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13372\\627695790.py\u001b[0m in \u001b[0;36mgradientDescent\u001b[1;34m(x, y, theta, alpha, num_iters, cost_history)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# calculate the cost function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# update the weights theta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "cost_history = []\n",
    "# J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500, cost_history)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 0.24216529\n",
    "theta = [7e-08, 0.0005239, -0.00055517]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, theta, freqs):\n",
    "    '''\n",
    "    Input:\n",
    "        - tweet: a string\n",
    "        - theta: a numpy array of weights of dimension (1,3)\n",
    "        - freqs: a dictionary of frequencies of the type\n",
    "        {\n",
    "            \"word\": {\n",
    "                \"positive\": int,\n",
    "                \"negative\": int\n",
    "            }\n",
    "        }\n",
    "    Output:\n",
    "        - prediction: a float\n",
    "    '''\n",
    "    x = extract_feature(tweet, freqs)\n",
    "    z = np.dot(x, theta)\n",
    "    prediction = sigmoid(z)\n",
    "    # Return flat prediction\n",
    "    return np.squeeze(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Me and my friends never talk ess-aych-eye-tee about anyone :)\n",
      "Prediction: 0.8238487737638809\n",
      "Tweet: Hate being the messenger :(\n",
      "Prediction: 0.11299084558460407\n",
      "Tweet: @junior_jones it was good as always 😀Flying back today though :(\n",
      "Prediction: 0.12059319801699146\n",
      "Tweet: 3 days without talking with Bae :(\n",
      "Prediction: 0.117450518622758\n",
      "Tweet: 24 hours not enough :(\n",
      "Prediction: 0.11439414552107792\n",
      "Tweet: @michaelsutthako morning , i miss you :( :*\n",
      "Prediction: 0.10506042699367006\n",
      "Tweet: @RickyBaby321 It's not forwarding Richard...it's 'follow' :)\n",
      "Prediction: 0.8293606842787128\n",
      "Tweet: @chewy4cutie but you can with me :)\n",
      "Prediction: 0.8248608247734253\n",
      "Tweet: 15 Days ago Danny took my wig and put it onto mark's head I want to go back there @thescript @TheScript_Danny :( ♥ https://t.co/9ojA3FPxKF\n",
      "Prediction: 0.10866962342466999\n",
      "Tweet: Some times I like this style :| but now I didn't liked it :( http://t.co/jjSI8VScPL\n",
      "Prediction: 0.11351574446987017\n"
     ]
    }
   ],
   "source": [
    "# Predict 10 random tweets\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(test_x))\n",
    "    tweet = test_x[idx]\n",
    "    prediction = predict_tweet(tweet, theta, freqs)\n",
    "    print(\"Tweet:\", tweet)\n",
    "    print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, theta, freqs):\n",
    "    '''\n",
    "    Input:\n",
    "        - test_x: a list of strings\n",
    "        - test_y: a list of labels\n",
    "        - theta: a numpy array of weights of dimension (1,3)\n",
    "        - freqs: a dictionary of frequencies of the type\n",
    "        {\n",
    "            \"word\": {\n",
    "                \"positive\": int,\n",
    "                \"negative\": int\n",
    "            }\n",
    "        }\n",
    "    Output:\n",
    "        - accuracy: a float\n",
    "    '''\n",
    "    y_hat = []\n",
    "    for tweet in test_x:\n",
    "        y_pred = predict_tweet(tweet, theta, freqs)\n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.9955\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of the model is\", test_logistic_regression(test_x, test_y, theta, freqs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84be8c82a7a3e86db451810d88cd8e67624a6eca0a3f1ff4d59d995ee7a917f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
