{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1: Classification and Vector Spaces\n",
    "# Week 1: Logistic Regression for Sentiment Analysis of Tweets\n",
    "## Own project: Analyzing the sentiment of my own tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download my own tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run assignment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter credentials\n",
    "api_keys = json.load(open('twitter-api-keys.local.json'))\n",
    "bearer_token = api_keys[\"bearer_token\"]\n",
    "client = tweepy.Client(\n",
    "    bearer_token=api_keys['bearer_token'], wait_on_rate_limit=True)\n",
    "\n",
    "auth = tweepy.OAuthHandler(\n",
    "    consumer_key=api_keys['api_key'],\n",
    "    consumer_secret=api_keys['api_key_secret'],\n",
    "    access_token=api_keys['access_token'],\n",
    "    access_token_secret=api_keys['access_token_secret'])\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = \"fcx_xm\"\n",
    "tweets = api.user_timeline(screen_name=me, count=1000)\n",
    "tweet_text = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating preprocessing functions for Spanish tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String cleaning function\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - string: a string to be cleaned\n",
    "    Output:\n",
    "        - cleaned_string: a clean string with the following actions:\n",
    "            - lowercase\n",
    "            - remove punctuation\n",
    "            - remove URLs, hashtags, mentions\n",
    "    \"\"\"\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    string = re.sub(r'RT', '', string)\n",
    "    string = re.sub(r'@\\S+', '', string)\n",
    "    string = re.sub(r'#', '', string)\n",
    "    string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    string = string.lower()\n",
    "    #string = unidecode.unidecode(string)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize(string):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - string: a string to be tokenized\n",
    "    Output:\n",
    "        - tokens: a list of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(string)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spanish stopwords function\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - tokens: a list of tokens\n",
    "    Output:\n",
    "        - tokens: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem function for spanish\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - tokens: a list of tokens\n",
    "    Output:\n",
    "        - tokens: a list of stems\n",
    "    \"\"\"\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "RT @lmprzz: objeto o pastel? \n",
      "dejé la facultad\n",
      "Cleaned text:\n",
      "  objeto o pastel \n",
      "deje la facultad\n",
      "Tokenized text:\n",
      "['objeto', 'o', 'pastel', 'deje', 'la', 'facultad']\n",
      "Removed stopwords:\n",
      "['objeto', 'pastel', 'deje', 'facultad']\n",
      "Stemmed text:\n",
      "['objet', 'pastel', 'dej', 'facult']\n"
     ]
    }
   ],
   "source": [
    "# Show transformation progress\n",
    "print('Original text:')\n",
    "print(tweet_text[0])\n",
    "print('Cleaned text:')\n",
    "print(clean_string(tweet_text[0]))\n",
    "print('Tokenized text:')\n",
    "print(tokenize(clean_string(tweet_text[0])))\n",
    "print('Removed stopwords:')\n",
    "print(remove_stopwords(tokenize(clean_string(tweet_text[0]))))\n",
    "print('Stemmed text:')\n",
    "print(stem(remove_stopwords(tokenize(clean_string(tweet_text[0])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire process of the string\n",
    "def process_text(tweet):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - tweet: a string to be processed\n",
    "    Output:\n",
    "        - tokens: a list of tokens\n",
    "    \"\"\"\n",
    "    tweet = clean_string(tweet)\n",
    "    tokens = tokenize(tweet)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['objet', 'pastel', 'dej', 'facult'], ['tra', 'w11'], ['tio', 'thinkp', 'x230'], [], ['madr', 'cabr', 'porq', 'lleg', 'tard', 'clas', 'mochil', 'cas']]\n"
     ]
    }
   ],
   "source": [
    "# Processed tweets\n",
    "processed_tweets = [process_text(tweet) for tweet in tweet_text]\n",
    "print(processed_tweets[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data for spanish positivity and negativity\n",
    "\n",
    "I will be using a dataset of positive and negative texts.\n",
    "\n",
    "URL: https://www.kaggle.com/datasets/luisdiegofv97/imdb-dataset-of-50k-movie-reviews-spanish?select=IMDB+Dataset+SPANISH.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb_dataset.csv\n",
    "# Keep columns review_es, sentimiento. In sentimiento, replace \"positivo\" with 1 and \"negativo\" with 0\n",
    "# data = pandas.read_csv('imdb_dataset.csv')\n",
    "# data = data[['review_es', 'sentimiento']]\n",
    "# data.sentimiento = data.sentimiento.replace(['positivo', 'negativo'], [1, 0])\n",
    "\n",
    "# Once the dataset has been cleaned\n",
    "data = pandas.read_csv('imdb_dataset_processed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in a new csv file\n",
    "data.to_csv('imdb_dataset_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           review_es  sentimiento\n",
      "0  Uno de los otros críticos ha mencionado que de...            1\n",
      "1  Una pequeña pequeña producción.La técnica de f...            1\n",
      "2  Pensé que esta era una manera maravillosa de p...            1\n",
      "3  Básicamente, hay una familia donde un niño peq...            0\n",
      "4  El \"amor en el tiempo\" de Petter Mattei es una...            1\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Create x and y lists\n",
    "x = data.review_es\n",
    "y = data.sentimiento\n",
    "\n",
    "# Print head 5 and len of both\n",
    "print(data.head(5))\n",
    "print(len(x))\n",
    "print(len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency distribution of the words in the reviews\n",
    "\n",
    "def build_freqs(train_x, train_y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - train_x: the list of reviews\n",
    "        - train_y: the list of labels\n",
    "    Output:\n",
    "        - freqs: a dictionary with the frequency of each word in the reviews\n",
    "\n",
    "    Dictionary structure:\n",
    "    {\n",
    "        'word1': {\n",
    "            'positive': int,\n",
    "            'negative': int\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    freqs = {}\n",
    "    for i in tqdm(range(len(train_x))):\n",
    "        tokens = process_text(train_x[i])\n",
    "        label = train_y[i]\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in freqs:\n",
    "                freqs[token] = {'positive': 0, 'negative': 0}\n",
    "            if label == 1:\n",
    "                freqs[token]['positive'] += 1\n",
    "            else:\n",
    "                freqs[token]['negative'] += 1\n",
    "\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f22f0b2a8c4748b45ec80e648a7fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Freq dict creation takes around 10 minutes. Save it to a file and load it later if it exists\n",
    "import os\n",
    "if os.path.exists('freqs.json'):\n",
    "    freqs = json.load(open('freqs.json'))\n",
    "else:\n",
    "    freqs = build_freqs(x, y)\n",
    "    json.dump(freqs, open('freqs.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pelicul', {'positive': 82967, 'negative': 90708}), ('mas', {'positive': 30251, 'negative': 27598}), ('the', {'positive': 29416, 'negative': 24283}), ('si', {'positive': 17832, 'negative': 22850}), ('hac', {'positive': 18234, 'negative': 19785}), ('pued', {'positive': 17054, 'negative': 18046}), ('sol', {'positive': 14071, 'negative': 17358}), ('buen', {'positive': 15215, 'negative': 15671}), ('ser', {'positive': 13122, 'negative': 15108}), ('histori', {'positive': 15469, 'negative': 12110})]\n"
     ]
    }
   ],
   "source": [
    "# Show sorted list of words and their frequencies\n",
    "print(sorted(freqs.items(), key=lambda x: x[1]['positive'] + x[1]['negative'], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Create train and test datasets\n",
    "train_x = x[:int(len(x) * 0.8)]\n",
    "train_y = y[:int(len(y) * 0.8)]\n",
    "test_x = x[int(len(x) * 0.8):]\n",
    "test_y = y[int(len(y) * 0.8):]\n",
    "\n",
    "# Print len of all\n",
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid, gradient descent, feature extraction, and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - x: a number\n",
    "    Output:\n",
    "        - sigmoid: the sigmoid of x\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction function\n",
    "def feature_extraction(text, freqs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - word: a text\n",
    "        - freqs: a dictionary with the frequency of each word in the reviews\n",
    "    Output:\n",
    "        - features: an array of (1, 3) with the following features:\n",
    "            - 1 = 1\n",
    "            - 2 = frequency of the tokens in the positive reviews\n",
    "            - 3 = frequency of the tokens in the negative reviews\n",
    "    \"\"\"\n",
    "    features = np.zeros((1, 3))\n",
    "    features[0, 0] = 1\n",
    "    tokens = process_text(text)\n",
    "    for token in tokens:\n",
    "        features[0][1] += freqs[token]['positive']\n",
    "        features[0][2] += freqs[token]['negative']\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction with 5 random reviews:\n",
      "Review: Chris y Andre son dos adolescentes promedio, ordinarios. Mal entendido por algunos y recogido por otros. Pero juntos están parados y todos pagarán. Juntos forman \"el ejército de dos\". Ellos incluyen y planifican \"cero día\". Ese día es cuando deciden asaltar su escuela secundaria e inevitablemente asesinar a 14 personas en sangre fría. Dijo a través de las cintas que hicieron \"Día cero\", apenas es un dicho ficticio de la tragedia de Columbine. \"Zero Día\" es una de esas películas que se meterán en la cabeza después. Los dos actores principales (Calvin Robertson y Andre Keuck) hacen un buen trabajo que sus personajes parecen casi cualquier adolescente sin marginismo que camina por la calle. Sus actuaciones eran muy creíbles, le gustaban un poco a estos tipos y que estaban aterrando. Disparó a un video casi totalmente de las perspectivas de los adolescentes \"Día cero\" se siente muy real y auténtico, como si estuvieras allí. Estos niños intentan racionalizar sus acciones al espectador y los actores lo venden. Pero se le advierte que sigue la tragedia de principio a fin y el final hace que sea impactante e incómodo para algunos.\n",
      "Features: [[1.00000e+00 3.73185e+05 4.04242e+05]]\n",
      "Review: Ante la cara, cualquier comedia adolescente corre el riesgo de sersofomórico y obvio, y lo suficientemente justo, la mayoría de ellos son. Hay algunos que han aumentado por encima de la banalidad habitual del material de origen (tráigalo, EUROTRIP), y me dan esperanza de que otros puedan unirse a la multitud deprimente de comedias adolescentes que realmente son divertidas y no solo una excusa para flashear. Algunas tetas de Starlet o ser un vehículo para un poco, fue como Tom Green.Enter aceptada, dirigida por John Cusack PAL Steve Pink y dirigida por la simpática Justin Long (el Smart Nerdy Kid en Galaxy Quest y más recientemente en Apple Computer Ads) , sobre un grupo de niños que no se aceptan en la universidad y deciden, como se describe en el remolque, para falsificar a un falso para que sus padres salgan de sus espaldas. Cuando van un poco demasiado lejos con el sitio web y otros niños terminan inscribiéndose, los niños tienen algunos problemas para resolver. La configuración es obviamente absurda, pero la mayoría de las comedias vive o muere en la ejecución, y aquí, aceptó hace bastante bien Las ganancias de Bartleby de Long (en su mayoría acortadas a 'B') están acompañadas por bastante bien intencionados amigos cómicamente aceptables para ayudarlo a compartir la carga, pero probablemente el mejor personaje de apoyo es 'Dean' Lewis (sin duda nombrado como homenaje a Rodney Dangerfield's Dean Martin), también conocido como tío Ben, Lewis Black, que está sabiamente restringido a los brezos cortos donde puede disparar rápidamente su marca única de humor observacional de malla. El negro es excelente aquí, llamando francamente a los niños a los que realmente es la vida (su línea final cargada de explosión es histérica) y brindando a la cara adulta a la Universidad Bogus para los padres. Pero mucho tiempo tiene que llevar la mayor parte de la película, y lo hace así, en espadas; Bartleby se identifica fácilmente con y lo llevamos casi de inmediato. Un buen giro fue proporcionado por las manos de los personajes de Columbus Short, un atleta que pierde su beca debido a una lesión y termina convirtiéndose en la facultad de arte de facto en la Fake College. También notable como la hermana pequeña y sarcástica hiper-inteligente y sarcástica de Bartleby es Hannah Marks.Yes, la película se siente familiar en algunos lugares; Hay una deuda aquí adeudada, de todos los lugares, para vengarse de los nerds. Los que son rechazados por todos los demás encuentran un hogar con Bartleby y sus cohortes, y por supuesto, los villanos son los tipos de carreras maestros conformistas mejorados por esteroides que dirigen una fraternidad (bueno, está bien, Frats son malvados) y busca humillar y enterrar a los bonos extraños. Por el - Gasp - Crime de ser diferente. Pero eso, a un lado, aceptado se divierte con el material, e incluso hace algunas preguntas decentes sobre las expectativas de los niños universitarios y el curso de la educación superior. No es que sea un teaser del cerebro de ninguna manera, pero aceptada no es solo otra comedia adolescente sin sentido (o vehículo Owen Wilson). Es un divertido y inteligente 90 minutos que, aunque no es una gran salida de su género, es lo suficientemente inteligente como lo suficientemente creativo como para ser muy agradable. Hay al menos un momento de brillo sin adulterar en la película, cuando Bartleby revisa una universidad cercana en un intento de cosechar ideas para el currículo de su propia escuela. Cuando ve que los niños están todos estresados ​​en la otra escuela (varias se niegan a hablar con él durante la clase por temor a perder algo), sacude la cabeza, pensando que debe haber una mejor manera. Mientras se mueve para irse, se encuentra con un flujo de niños que van por el otro lado, una persona se mueve contra una marea humana, mientras que en el fondo, las líneas de apertura de Eleanor Rigby juegan. Tal vez un momento no planificado de brillantez, pero la brillantez sin menos.\n",
      "Features: [[1.000000e+00 1.155523e+06 1.176407e+06]]\n",
      "Review: Wow, esta fue otra buena giro de la tarta estadounidense original, no tan buena como el campamento de banda, pero definitivamente mucho mejor la milla desnuda.Dwight y Erik Stiller lideran la comedia en este, pero en realidad preferí el diálogo en este a la milla desnuda.El guión fue escrito mucho mejor y la comedia fluía más suavemente, sin embargo, la mayor parte de la comedia vino del sexo, pero eso está bien porque es por eso que vemos estas películas de todos modos, ¿de todos modos?La Roca Midget también tenía un cameo realmente bueno, considerando el intenso esfuerzo dado por él a la milla desnuda, su escena con astilladora era increíble y me hizo reír mi culo cuando lo vi. La película fue una mejora definitiva en mi opinión en comparación.A la milla desnuda, si te gustó cualquier película de pastel estadounidenses anteriores, debe gustarle la casa beta, a menos que vea todos los spin-offs de los pasteles estadounidenses un desperdicio de dinero.\n",
      "Features: [[1.00000e+00 5.56826e+05 5.78479e+05]]\n",
      "Review: Esta película realmente demuestra que el mundo es con demasiada frecuencia un lugar injusto, especialmente en el mundo de las imágenes en movimiento. \"La asignación\" recibió apenas atención sobre su lanzamiento y no se sorprendió en la taquilla, pero cuando la historia se escribirá, esta película seguramente recibirá un alabado perdido durante mucho tiempo. Gracias a Dios, estoy rodeado de amigos que saben lo que es bueno. para mi. Al ser un aficionado a la película como yo, un amigo altamente recomendado \"la tarea\", una película en la que ni siquiera había escuchado. Decidí revisar lo que Leonard Maltin lo dio, y no es sorprendente que lo hiciera ** 1/2. Sabiendo que este es el mismo grado que le dio a los clásicos como \"alien\", \"los sospechosos habituales\" y \"la matriz\" (no te deseo). Sabía que su significado no quiso decir que Diddly Squat Jack. Así que sin dudar de que salí y lo compré en DVD. Esto fue hace unos 3 años y la película sigue siendo una de mis pertenencias más orgullosas en mi colección de DVD, a pesar de un diseño de portada que hace eco de un apestista de bajo presupuesto con Casper Van Dien. \"La tarea\" se dirige de manera experta, brindando algunos momentos realmente intensos que Te mantendrá al borde de su asiento a lo largo de la película, además de que cuenta con una historia brillante que usted sabe, será plagada con giros y giros inesperados. Estrella Aidan Quinn en una de sus mejores actuaciones, y lo sirve con un gran apoyo de Donald Sutherland y Ben Kingsley, que están en gran forma, como 40 de 42 comentarios de los usuarios como esta película, la mayoría de ellos no puede parecer que Alabado lo suficiente. ¿Entonces, Qué esperas? Si se llama a ti mismo un fanático de la acción, los thrillers, deberías haberlo comprado, lo alquiló, lo he visto ayer.\n",
      "Features: [[1.000000e+00 1.058099e+06 1.114019e+06]]\n",
      "Review: Tipo de las escenas eróticas, solo para darte cuenta de que era uno de los bits de película más aficionados e increíbles que he visto.Tipo de un proyecto de cine de secundaria.¿Qué pensaba Rosanna Arquette?¿Y qué fue con todos esos personajes de acciones en ese extraño pueblo medio oeste?Bastante difícil de involucrarse con este.No hay lecciones que aprender de ella, no hay ideas brillantes, simplemente íntimas y bastante ridículas (pero mucha piel, si eso lo intriga) grabó en video sin sentido ... lo que fue con la relación bisexual, de la nada, después de todos los encuentros heterosexuales..¿Y qué estaba con esa danza absurda, con todos jugando sus roles estereotipados?Dé a este pase, es como un millón de otras millas de películas malas, desperdiciadas, dinero que podría haberse gastado en niños de hambre o SIDA en África ...\n",
      "Features: [[1.00000e+00 3.62556e+05 4.10472e+05]]\n"
     ]
    }
   ],
   "source": [
    "# Test feature extraction with 5 random reviews\n",
    "print('Feature extraction with 5 random reviews:')\n",
    "idx = np.random.randint(0, len(x), 5)\n",
    "for i in idx:\n",
    "    print(\"Review:\", x[i])\n",
    "    # Print in scientific notation\n",
    "    print(\"Features:\", feature_extraction(x[i], freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.7310585786300049\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14196\\2883066539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtmp_J\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_theta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cost:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_J\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Theta:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Test sigmoid and gradient\n",
    "print(sigmoid(0))\n",
    "print(sigmoid(1))\n",
    "\n",
    "tmp_x = np.append(np.ones((10, 1)), np.random.randn(10, 2), axis=1)\n",
    "tmp_x\n",
    "tmp_y = (np.random.randn(10, 1) > 0.35).astype(float)\n",
    "tmp_y\n",
    "\n",
    "tmp_J, tmp_theta = gradient_descent(tmp_x, tmp_y, np.zeros((3, 1)), 0.01, 1000)\n",
    "print(\"Cost:\", tmp_J[-1]) \n",
    "print(\"Theta:\", tmp_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14196\\1180876727.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract the features from the training set\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in tqdm(range(len(train_x))):\n",
    "    X[i] = feature_extraction(train_x[i], freqs)\n",
    "\n",
    "Y = train_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeb7f191f0e35c9beeb8ff0cea8f72192bbcb4fd900e19ae06dfc5abae03ecc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
