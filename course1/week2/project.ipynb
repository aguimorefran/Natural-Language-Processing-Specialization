{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1: Classification and Vector Spaces\n",
    "# Own project 2: Classifying a conversation using Naive Bayes\n",
    "\n",
    "In this notebook I will apply the knowledge I gained from the previous notebooks to classify a conversation using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training data\n",
    "\n",
    "### 1.1 Importing and cleaning\n",
    "\n",
    "For this own project (in spanish), the first problem is getting a spanish dataset that we can use to train our model.\n",
    "For that, I will use a pre labeled dataset of spanish tweets.\n",
    "\n",
    "Dataset source: http://tass.sepln.org/tass_data/download.php?auth=6tQassjsi4HeLvhe5rN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: general-train-tagged-3l.xml\n",
      "Parsing file: general-train-tagged.xml\n",
      "Parsing file: intertass-CR-train-tagged.xml\n",
      "Parsing file: intertass-ES-train-tagged.xml\n",
      "Parsing file: TASS2019_country_CR_train.xml\n",
      "Parsing file: TASS2019_country_ES_train.xml\n",
      "Parsing file: TASS2019_country_MX_train.xml\n",
      "Parsing file: TASS2019_country_PE_train.xml\n",
      "Parsing file: TASS2019_country_UY_train.xml\n"
     ]
    }
   ],
   "source": [
    "# Folder containing xml files: ../../datasets/es\n",
    "# All xml have the structure\n",
    "# <tweet>\n",
    "# \t\t<tweetid>768220253730009091</tweetid>\n",
    "# \t\t<user>Alis_8496</user>\n",
    "# \t\t<content>@Yulian_Poe @guillermoterry1 Ah. mucho m√°s por supuesto! solo que lo incluyo. Me hab√≠as entendido mal </content>\n",
    "# \t\t<date>2016-08-23 22:55:55</date>\n",
    "# \t\t<lang>es</lang>\n",
    "# \t\t<sentiment>\n",
    "# \t\t\t<polarity><value>P</value></polarity>\n",
    "# \t\t</sentiment>\n",
    "# \t</tweet>\n",
    "\n",
    "# Read all xml into a pd dataframe. Keep only content and polarity value\n",
    "folder = '../../datasets/es/'\n",
    "files = os.listdir(folder)\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    if file.endswith(\".xml\"):\n",
    "        print(\"Parsing file: \" + file)\n",
    "        tree = ET.parse(folder + file)\n",
    "        root = tree.getroot()\n",
    "        for tweet in root:\n",
    "            content = tweet.find('content')\n",
    "            polarity = tweet.find('sentiment/polarity/value')\n",
    "            if content is not None and polarity is not None:\n",
    "                df = df.append({'content': content.text, 'polarity': polarity.text}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@NoilyMV yo soy totalmente puntual</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DexterAOM por eso es que se dice lo del final...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@robertobrenes Bueno, no es tanto lo mayor com...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@doriamdiaz El de Halfon de Germinal se ve mor...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahorita me van a cambiar las ligas de los brak...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content polarity\n",
       "0                 @NoilyMV yo soy totalmente puntual     NONE\n",
       "1  @DexterAOM por eso es que se dice lo del final...      NEU\n",
       "2  @robertobrenes Bueno, no es tanto lo mayor com...        N\n",
       "3  @doriamdiaz El de Halfon de Germinal se ve mor...        P\n",
       "4  Ahorita me van a cambiar las ligas de los brak...     NONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    2613\n",
       "P    1941\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove NONE polarity\n",
    "df = df[df['polarity'] != 'NONE']\n",
    "df['polarity'].value_counts()\n",
    "\n",
    "# Remove NEUTRAL polarity\n",
    "df = df[df['polarity'] != 'NEU']\n",
    "df['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive tweets: 1941\n",
      "Length of negative tweets: 1941\n"
     ]
    }
   ],
   "source": [
    "# Separate into positive and negative tweets\n",
    "train_x_pos = df[df['polarity'] == 'P']\n",
    "train_x_neg = df[df['polarity'] == 'N']\n",
    "\n",
    "# Remove the difference in length between positive and negative tweets\n",
    "len_final = len(train_x_pos) if len(train_x_pos) < len(train_x_neg) else len(train_x_neg)\n",
    "train_x_pos = train_x_pos[:len_final]\n",
    "train_x_neg = train_x_neg[:len_final]\n",
    "\n",
    "# Keep only the content, transform content column to list\n",
    "train_x_pos = train_x_pos['content'].tolist()\n",
    "train_x_neg = train_x_neg['content'].tolist()\n",
    "\n",
    "# Print length of positive and negative tweets\n",
    "print(\"Length of positive tweets: \" + str(len(train_x_pos)))\n",
    "print(\"Length of negative tweets: \" + str(len(train_x_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uno de los otros cr√≠ticos ha mencionado que de...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Una peque√±a peque√±a producci√≥n.La t√©cnica de f...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pens√© que esta era una manera maravillosa de p...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B√°sicamente, hay una familia donde un ni√±o peq...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El \"amor en el tiempo\" de Petter Mattei es una...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_es sentimiento\n",
       "0  Uno de los otros cr√≠ticos ha mencionado que de...    positivo\n",
       "1  Una peque√±a peque√±a producci√≥n.La t√©cnica de f...    positivo\n",
       "2  Pens√© que esta era una manera maravillosa de p...    positivo\n",
       "3  B√°sicamente, hay una familia donde un ni√±o peq...    negativo\n",
       "4  El \"amor en el tiempo\" de Petter Mattei es una...    positivo"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import imdb_clean.csv\n",
    "df_imdb = pd.read_csv(folder + 'imdb_clean.csv')\n",
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the IMDB dataset into positive and negative tweets\n",
    "imdb_pos = df_imdb[df_imdb['sentimiento'] == 'positivo']['review_es'].tolist()\n",
    "imdb_neg = df_imdb[df_imdb['sentimiento'] == 'negativo']['review_es'].tolist()\n",
    "\n",
    "# Append to the existing pos and neg lists\n",
    "train_x_pos.extend(imdb_pos)\n",
    "train_x_neg.extend(imdb_neg)\n",
    "\n",
    "# Create the final training set\n",
    "train_x = train_x_pos + train_x_neg\n",
    "train_y = [1] * len(train_x_pos) + [0] * len(train_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x160ba10dc88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOaklEQVR4nO3df6zdd13H8efLloEIMqAXMttqS1LUhgDCdYzgj4nC2kFsTPijQx0sJM0iMxj/kC5EjOEfMNGAYVAbbAhRKTEMqVCcBET+ILDdwgYro1DGYNcSeyeKBv6Yhbd/nG/h7O703tNy2tv79vlITu73+/l+zjnvd3P66vd+z/fbb6oKSdL692NrXYAkaTYMdElqwkCXpCYMdElqwkCXpCY2rtUbb9q0qbZt27ZWby9J69KxY8ceqqq5SdvWLNC3bdvGwsLCWr29JK1LSb5+rm0ecpGkJgx0SWrCQJekJgx0SWrCQJekJlY9yyXJIeDlwOmqetaE7QHeBlwPfBd4dVV9dtaFAmzb/+GL8bLSih5488vWugRpKtPsob8b2LXC9t3AjuGxD3jnj17WoxnmWit+9rRerBroVfVJ4FsrTNkDvKdGPg1cmeSqWRUoSZrOLI6hbwYeHFtfHMYeJcm+JAtJFpaWlmbw1pKks2YR6JkwNvGuGVV1sKrmq2p+bm7ilauSpAs0i0BfBLaOrW8BTs3gdSVJ52EWgX4EuDEj1wDfrqpvzuB1H8EzDbRW/OxpvZjmtMX3AtcCm5IsAn8CPAagqg4ARxmdsniS0WmLN12sYv2LJUnntmqgV9UNq2wv4LUzq0iSdEG8UlSSmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12Smpgq0JPsSnIiyckk+ydsf1KSf0xyT5LjSW6afamSpJWsGuhJNgC3AbuBncANSXYum/Za4ItV9RzgWuDPk1wx41olSSuYZg/9auBkVd1fVQ8Dh4E9y+YU8MQkAZ4AfAs4M9NKJUkrmibQNwMPjq0vDmPj3g78PHAK+ALwuqr6/vIXSrIvyUKShaWlpQssWZI0yTSBngljtWz9OuBu4KeA5wJvT/KTj3pS1cGqmq+q+bm5ufMsVZK0kmkCfRHYOra+hdGe+LibgNtr5CTwNeDnZlOiJGka0wT6XcCOJNuHLzr3AkeWzfkG8OsASZ4O/Cxw/ywLlSStbONqE6rqTJJbgDuADcChqjqe5OZh+wHgTcC7k3yB0SGa11fVQxexbknSMqsGOkBVHQWOLhs7MLZ8CnjpbEuTJJ0PrxSVpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqYqpAT7IryYkkJ5PsP8eca5PcneR4kn+dbZmSpNVsXG1Ckg3AbcBLgEXgriRHquqLY3OuBN4B7KqqbyR52kWqV5J0DtPsoV8NnKyq+6vqYeAwsGfZnFcCt1fVNwCq6vRsy5QkrWaaQN8MPDi2vjiMjXsm8OQkn0hyLMmNk14oyb4kC0kWlpaWLqxiSdJE0wR6JozVsvWNwPOBlwHXAX+c5JmPelLVwaqar6r5ubm58y5WknRuqx5DZ7RHvnVsfQtwasKch6rqO8B3knwSeA7w5ZlUKUla1TR76HcBO5JsT3IFsBc4smzOB4FfTrIxyeOBFwD3zbZUSdJKVt1Dr6ozSW4B7gA2AIeq6niSm4ftB6rqviT/BHwe+D7wrqq692IWLkl6pFQtPxx+aczPz9fCwsKavLckrVdJjlXV/KRtXikqSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU1MFehJdiU5keRkkv0rzPvFJN9L8orZlShJmsaqgZ5kA3AbsBvYCdyQZOc55r0FuGPWRUqSVjfNHvrVwMmqur+qHgYOA3smzPt94P3A6RnWJ0ma0jSBvhl4cGx9cRj7gSSbgd8CDqz0Qkn2JVlIsrC0tHS+tUqSVjBNoGfCWC1bfyvw+qr63kovVFUHq2q+qubn5uamLFGSNI2NU8xZBLaOrW8BTi2bMw8cTgKwCbg+yZmq+odZFClJWt00gX4XsCPJduDfgL3AK8cnVNX2s8tJ3g18yDCXpEtr1UCvqjNJbmF09soG4FBVHU9y87B9xePmkqRLY5o9dKrqKHB02djEIK+qV//oZUmSzpdXikpSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDUxVaAn2ZXkRJKTSfZP2P7bST4/PD6V5DmzL1WStJJVAz3JBuA2YDewE7ghyc5l074G/GpVPRt4E3Bw1oVKklY2zR761cDJqrq/qh4GDgN7xidU1aeq6j+H1U8DW2ZbpiRpNdME+mbgwbH1xWHsXF4DfGTShiT7kiwkWVhaWpq+SknSqqYJ9EwYq4kTk19jFOivn7S9qg5W1XxVzc/NzU1fpSRpVRunmLMIbB1b3wKcWj4pybOBdwG7q+o/ZlOeJGla0+yh3wXsSLI9yRXAXuDI+IQkPw3cDvxuVX159mVKklaz6h56VZ1JcgtwB7ABOFRVx5PcPGw/ALwReCrwjiQAZ6pq/uKVLUlaLlUTD4dfdPPz87WwsLAm7y1J61WSY+faYfZKUUlqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqYuM0k5LsAt4GbADeVVVvXrY9w/brge8Cr66qz864Vumi27b/w2tdgv6feeDNL5vZa626h55kA3AbsBvYCdyQZOeyabuBHcNjH/DOmVUoXSKGudbCLD930xxyuRo4WVX3V9XDwGFgz7I5e4D31MingSuTXDWzKiVJq5om0DcDD46tLw5j5zuHJPuSLCRZWFpaOt9aJUkrmCbQM2GsLmAOVXWwquaran5ubm6a+iRJU5om0BeBrWPrW4BTFzBHknQRTRPodwE7kmxPcgWwFziybM4R4MaMXAN8u6q+OeNapYtqlmcbSNOa5edu1dMWq+pMkluAOxidtnioqo4nuXnYfgA4yuiUxZOMTlu8aWYVSpeQoa71bKrz0KvqKKPQHh87MLZcwGtnW5ok6Xx4pagkNWGgS1ITBrokNWGgS1ITGX2fuQZvnCwBX7/Ap28CHpphOZej7j127w/699i9P7g8e/yZqpp4ZeaaBfqPIslCVc2vdR0XU/ceu/cH/Xvs3h+svx495CJJTRjoktTEeg30g2tdwCXQvcfu/UH/Hrv3B+usx3V5DF2S9GjrdQ9dkrSMgS5JTay7QE+yK8mJJCeT7F/relaS5FCS00nuHRt7SpKPJvnK8PPJY9tuHfo6keS6sfHnJ/nCsO0vh5tyk+SxSd43jH8mybZL3N/WJP+S5L4kx5O8rmGPj0tyZ5J7hh7/tFuPQw0bknwuyYea9vfAUNvdSRY69ghAVa2bB6P/vverwDOAK4B7gJ1rXdcK9f4K8Dzg3rGxPwP2D8v7gbcMyzuHfh4LbB/63DBsuxN4IaM7Q30E2D2M/x5wYFjeC7zvEvd3FfC8YfmJwJeHPjr1GOAJw/JjgM8A13TqcXjfPwT+DvhQt8/p8L4PAJuWjbXqsarWXaC/ELhjbP1W4Na1rmuVmrfxyEA/AVw1LF8FnJjUC6P/f/6Fw5wvjY3fAPzV+JxheSOjK9qyhr1+EHhJ1x6BxwOfBV7QqUdGdxj7GPBifhjobfob3vcBHh3orXqsqnV3yGWqm1Ff5p5ew92chp9PG8bP1dvmYXn5+COeU1VngG8DT71ola9g+BXzFxjtwbbqcTgccTdwGvhoVXXr8a3AHwHfHxvr1B+M7nH8z0mOJdk3jHXrcbobXFxGproZ9Tp1rt5W6vmy+PNI8gTg/cAfVNV/D4cVJ06dMHbZ91hV3wOem+RK4ANJnrXC9HXVY5KXA6er6liSa6d5yoSxy7a/MS+qqlNJngZ8NMmXVpi7Xntcd3voHW5G/e9JrgIYfp4exs/V2+KwvHz8Ec9JshF4EvCti1b5BEkewyjM/7aqbh+GW/V4VlX9F/AJYBd9enwR8JtJHgAOAy9O8jf06Q+Aqjo1/DwNfAC4mmY9wvoL9GluWH25OwK8alh+FaPjzmfH9w7flm8HdgB3Dr8K/k+Sa4Zv1G9c9pyzr/UK4OM1HMS7FIZ6/hq4r6r+YmxTpx7nhj1zkvw48BvAl2jSY1XdWlVbqmobo79PH6+q36FJfwBJfiLJE88uAy8F7qVRjz9wqQ/az+DLjesZnU3xVeANa13PKrW+F/gm8L+M/gV/DaPjah8DvjL8fMrY/DcMfZ1g+PZ8GJ9n9AH8KvB2fniF7+OAv2d0c+47gWdc4v5+idGvlZ8H7h4e1zfr8dnA54Ye7wXeOIy36XGsvmv54ZeibfpjdFbcPcPj+Nnc6NTj2YeX/ktSE+vtkIsk6RwMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCb+D/b9pXaDZOafAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot train y over indexes. points\n",
    "plt.scatter(range(len(train_y)), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle train_x and train_y. Transform first to pandas, shuffle and transform back to list\n",
    "df_shuffle = pd.DataFrame({'x': train_x, 'y': train_y})\n",
    "df_shuffle = df_shuffle.reindex(np.random.permutation(df_shuffle.index))\n",
    "train_x = df_shuffle['x'].tolist()\n",
    "train_y = df_shuffle['y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x160bae38f08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOk0lEQVR4nO3df6zdd13H8efLloEIMmAXMttpSyxiQwDhOkbwx0SBtiMuJvyxoQ4WkmaRGYx/SBcixhACmGjAMKgNLoSolBiGVChOAiJ/ENhuYYOVrXAZg11L7J0oGoiZHW//ON/C2d3pvafbaW/vO89HcnK+38/3c77n/V661/3e7/l+70lVIUna+H5svQuQJM2GgS5JTRjoktSEgS5JTRjoktTE5vV644suuqi2bdu2Xm8vSRvSkSNH7q+quUnb1i3Qt23bxsLCwnq9vSRtSEm+ebptnnKRpCYMdElqwkCXpCYMdElqwkCXpCbWvMolyU3AK4ATVfXsCdsDvBPYA3wfeE1VfWHWhQI8642H+d8H/WNikvq4921XzGxf0xyhvw/Ytcr23cCO4bEXeM+jL+vhDHNJHW3b97GZ7WvNQK+qzwDfWWXKlcD7a+RzwIVJLp5VgacY5pK0ulmcQ98C3De2vjSMPUySvUkWkiwsLy/P4K0lSafMItAzYWzi4XRVHaiq+aqan5ubeOeqJOkRmkWgLwGXjK1vBY7PYL8P8bhNk35uSJJOmUWgHwKuychlwHer6tsz2O9D3P2WPYa6pHZmeZXLNJctfgC4HLgoyRLwJ8BjAKpqP3CY0SWLi4wuW7x2ZtWtcPdb9pytXUvShrdmoFfV1WtsL+B1M6tIkvSIeKeoJDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDUxVaAn2ZXkWJLFJPsmbH9Skn9MckeSo0munX2pkqTVrBnoSTYBNwK7gZ3A1Ul2rpj2OuArVfVc4HLgz5NcMONaJUmrmOYI/VJgsaruqaoHgIPAlSvmFPDEJAGeAHwHODnTSiVJq5om0LcA942tLw1j494F/DxwHPgy8Pqq+sHKHSXZm2QhycLy8vIjLFmSNMk0gZ4JY7Vi/eXA7cBPAc8D3pXkJx/2oqoDVTVfVfNzc3NnWKokaTXTBPoScMnY+lZGR+LjrgVurpFF4BvAs2ZToiRpGtME+m3AjiTbhw86rwIOrZjzLeDXAZI8Hfg54J5ZFipJWt3mtSZU1ckk1wO3AJuAm6rqaJLrhu37gTcD70vyZUanaN5QVfefxbolSSusGegAVXUYOLxibP/Y8nHgZbMtTZJ0JrxTVJKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqYmpAj3JriTHkiwm2XeaOZcnuT3J0ST/OtsyJUlr2bzWhCSbgBuBlwJLwG1JDlXVV8bmXAi8G9hVVd9K8rSzVK8k6TSmOUK/FFisqnuq6gHgIHDlijmvAm6uqm8BVNWJ2ZYpSVrLNIG+BbhvbH1pGBv3TODJST6d5EiSaybtKMneJAtJFpaXlx9ZxZKkiaYJ9EwYqxXrm4EXAFcALwf+OMkzH/aiqgNVNV9V83Nzc2dcrCTp9NY8h87oiPySsfWtwPEJc+6vqu8B30vyGeC5wFdnUqUkaU3THKHfBuxIsj3JBcBVwKEVcz4C/HKSzUkeD7wQuGu2pUqSVrPmEXpVnUxyPXALsAm4qaqOJrlu2L6/qu5K8k/Al4AfAO+tqjvPZuGSpIdK1crT4efG/Px8LSwsrMt7S9JGleRIVc1P2uadopLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUhIEuSU0Y6JLUxFSBnmRXkmNJFpPsW2XeLyZ5MMkrZ1eiJGkaawZ6kk3AjcBuYCdwdZKdp5n3duCWWRcpSVrbNEfolwKLVXVPVT0AHASunDDv94EPASdmWJ8kaUrTBPoW4L6x9aVh7IeSbAF+C9i/2o6S7E2ykGRheXn5TGuVJK1imkDPhLFasf4O4A1V9eBqO6qqA1U1X1Xzc3NzU5YoSZrG5inmLAGXjK1vBY6vmDMPHEwCcBGwJ8nJqvqHWRQpSVrbNIF+G7AjyXbg34CrgFeNT6iq7aeWk7wP+KhhLknn1pqBXlUnk1zP6OqVTcBNVXU0yXXD9lXPm0uSzo1pjtCpqsPA4RVjE4O8ql7z6MuSJJ0p7xSVpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqYqpAT7IrybEki0n2Tdj+20m+NDw+m+S5sy9VkrSaNQM9ySbgRmA3sBO4OsnOFdO+AfxqVT0HeDNwYNaFSpJWN80R+qXAYlXdU1UPAAeBK8cnVNVnq+o/h9XPAVtnW6YkaS3TBPoW4L6x9aVh7HReC3x80oYke5MsJFlYXl6evkpJ0pqmCfRMGKuJE5NfYxTob5i0vaoOVNV8Vc3Pzc1NX6UkaU2bp5izBFwytr4VOL5yUpLnAO8FdlfVf8ymPEnStKY5Qr8N2JFke5ILgKuAQ+MTkvw0cDPwu1X11dmXKUlay5pH6FV1Msn1wC3AJuCmqjqa5Lph+37gTcBTgXcnAThZVfNnr2xJ0kqpmng6/Kybn5+vhYWFdXlvSdqokhw53QGzd4pKUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhMGuiQ1YaBLUhObp5mUZBfwTmAT8N6qetuK7Rm27wG+D7ymqr4w41r52Rs+xsma9V4laf3c+7YrZravNY/Qk2wCbgR2AzuBq5PsXDFtN7BjeOwF3jOzCgeGuaSOtu372Mz2Nc0pl0uBxaq6p6oeAA4CV66YcyXw/hr5HHBhkotnViUY5pK0hmkCfQtw39j60jB2pnNIsjfJQpKF5eXlM61VkrSKaQI9E8ZWHi9PM4eqOlBV81U1Pzc3N019kqQpTRPoS8AlY+tbgeOPYM6jsnnSjwxJ0g9NE+i3ATuSbE9yAXAVcGjFnEPANRm5DPhuVX17loUuvvUKQ11SO7O8ymXNyxar6mSS64FbGF22eFNVHU1y3bB9P3CY0SWLi4wuW7x2ZhWOWXzr7BqXpG6mug69qg4zCu3xsf1jywW8bralSZLOhHeKSlITBrokNWGgS1ITBrokNZHR55nr8MbJMvDNR/jyi4D7Z1jO+ah7j937g/49du8Pzs8ef6aqJt6ZuW6B/mgkWaiq+fWu42zq3mP3/qB/j937g43Xo6dcJKkJA12SmtiogX5gvQs4B7r32L0/6N9j9/5gg/W4Ic+hS5IebqMeoUuSVjDQJamJDRfoSXYlOZZkMcm+9a5nNUluSnIiyZ1jY09J8okkXxuenzy27Yahr2NJXj42/oIkXx62/eXwpdwkeWySDw7jn0+y7Rz3d0mSf0lyV5KjSV7fsMfHJbk1yR1Dj3/arcehhk1Jvpjko037u3eo7fYkCx17BKCqNsyD0Z/v/TrwDOAC4A5g53rXtUq9vwI8H7hzbOzPgH3D8j7g7cPyzqGfxwLbhz43DdtuBV7E6JuhPg7sHsZ/D9g/LF8FfPAc93cx8Pxh+YnAV4c+OvUY4AnD8mOAzwOXdepxeN8/BP4O+Gi3f6fD+94LXLRirFWPVbXhAv1FwC1j6zcAN6x3XWvUvI2HBvox4OJh+WLg2KReGP39+RcNc+4eG78a+KvxOcPyZkZ3tGUde/0I8NKuPQKPB74AvLBTj4y+YeyTwEv4UaC36W9433t5eKC36rGqNtwpl6m+jPo89/Qavs1peH7aMH663rYMyyvHH/KaqjoJfBd46lmrfBXDr5i/wOgItlWPw+mI24ETwCeqqluP7wD+CPjB2Fin/mD0Hcf/nORIkr3DWLcep/uCi/PIVF9GvUGdrrfVej4v/nskeQLwIeAPquq/h9OKE6dOGDvve6yqB4HnJbkQ+HCSZ68yfUP1mOQVwImqOpLk8mleMmHsvO1vzIur6niSpwGfSHL3KnM3ao8b7gj9rH8Z9Tnw70kuBhieTwzjp+ttaVheOf6Q1yTZDDwJ+M5Zq3yCJI9hFOZ/W1U3D8Otejylqv4L+DSwiz49vhj4zST3AgeBlyT5G/r0B0BVHR+eTwAfBi6lWY+w8QJ9mi+sPt8dAl49LL+a0XnnU+NXDZ+Wbwd2ALcOvwr+T5LLhk/Ur1nxmlP7eiXwqRpO4p0LQz1/DdxVVX8xtqlTj3PDkTlJfhz4DeBumvRYVTdU1daq2sbo/6dPVdXv0KQ/gCQ/keSJp5aBlwF30qjHHzrXJ+1n8OHGHkZXU3wdeON617NGrR8Avg38H6Of4K9ldF7tk8DXhuenjM1/49DXMYZPz4fxeUb/AL8OvIsf3eH7OODvGX05963AM85xf7/E6NfKLwG3D489zXp8DvDFocc7gTcN4216HKvvcn70oWib/hhdFXfH8Dh6Kjc69Xjq4a3/ktTERjvlIkk6DQNdkpow0CWpCQNdkpow0CWpCQNdkpow0CWpif8HDR+0ZVjIkFYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(train_y)), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of the training set x: \n",
      "[\"Si no has visto a Zombie Bloodbath, no lo has hecho. Un concurso como 'Haz tu propia pel√≠cula de terror en un d√≠a', no podr√≠a ocurrir una entrada que esta indignaci√≥n de un insulto en la inteligencia de cualquier espectador. El Sr. Hoets olvid√≥ una historia, una trama, un di√°logo adecuado, el hecho de que las personas necesitan algunos talentos de acci√≥n b√°sica y el dise√±ador de iluminaci√≥n acreditado obviamente se olvid√≥ de aparecer. Parece que se registra en la crujiente de Handycams, y copiado en un equipo peor a√∫n. El efecto de maquillaje consiste en la m√°scara negra para que los zombies y el yogur se vierten sobre las cabezas de las personas para simular su fundici√≥n de la piel. Esto no es m√°s que una pel√≠cula en casa, y una muy mala tambi√©n. Solo divertido para observar a los amigos, familiares y vecinos que estaban dispuestos a aparecer por la filmaci√≥n. No puedo por la vida de m√≠ entender por qu√© esta burla de un producto se encuentra en cualquier revista o sitio web de cine seria, tengo pel√≠culas caseras de bodas que son mucho mejor y m√°s interesantes. Una p√©rdida total de tiempo, dinero y energ√≠a. La secuela Zombie Bloodbath II es m√°s de la misma basura.\", '¬øQui√©n fue George C. Scott? George C. Scott fue un actor de renombre. Pr√°cticamente cualquier pel√≠cula en la que ha estado es el mejor para ello. Ahora, Ol \\'George no ten√≠a absolutamente nada que ver con esta pel√≠cula ..., pero una vez dijo algo que describe dicha pel√≠cula a un TI, no recuerdas sus palabras exactas, pero b√°sicamente dijo que gran escritura puede ahorrar mala actuaci√≥n, pero genial Actuar no puede salvar mal escritura. Nunca ha sido esta peque√±a observaci√≥n m√°s verdadera que en \"Las nuevas aventuras de Laurel y Hardy: para el amor o la momia\". El casting de los dos clientes potenciales era absolutamente perfecto. Bronson Pinchot (Laurel) y Gailard Sartain (Hardy) no solo miran las partes, sino que hacen un trabajo excepcionalmente bueno al imitar el verdadero trato (modales y todos). Esta pel√≠cula debe pararse como un testimonio duradero de sus talentos. Dicho esto, esta pel√≠cula cae en su cara cuando se trata de la escritura (lo adivinaste). Aseguida desde el di√°logo de apertura entre Pinchot y Sartain (que fue muy \"en car√°cter\") y una breve gordita que involucra un taxi, esta pel√≠cula es Una tarea absoluta para sentarse a trav√©s de.Problem # 1: demasiado tiempo y esfuerzo entr√≥ en la trama. No quiero saber por qu√© la momia quiere secuestrar a la hermosa dama brit√°nica. Lo que quiero es ver a STAN y Ollie (o al menos, sus stand-ins). Demasiado tiempo de pantalla se dedic√≥ a explicar la trama o a los personajes secundarios no muy divertidos que dichos gr√°ficos giraban alrededor. Sin embargo, incluso si esta pel√≠cula hab√≠a sido todas las bromas, que a√∫n nos dejar√≠a con ...... Problema # 2: La mayor√≠a de los chistes son lo que llamar√≠a la slapstick \"diluido\". ¬øQu√© quiere decir con \"diluido\"? En Slapstick, un personaje se lastima en una forma exagerada para el efecto comedico (Ala Looney Tunes, 3 Stooges ..., ¬øqu√© hay de Laurel & Hardy?). En \"diluido\" Slapstick (a medida que lo defino), un personaje se duele ligeramente o los inconvenientes, y los cineastas juegan eso para el efecto comedico. Una ilustraci√≥n ayudar√≠a: en Looney Tunes, Daffy Duck recibe un disparo por Elmer Fudd. Su cuenta se cae y √©l lo pone de nuevo. Ese es la slapstick cl√°sica. Esta \"gema\", Ollie se golpea accidentalmente a algunas personas. Se dan la vuelta, diles que tenga cuidado y contin√∫e por su Feliz Way. Eso no es slapstick. Eso ni siquiera es gracioso. Eso es solo ... aburrido ... y esta pel√≠cula est√° llena de este tipo de chistes. Es como si fueran pan y mantequilla de esta pel√≠cula. Los escritores y directores solo toman estos momentos aburridos y act√∫an como si se supone que son divertidos. Por supuesto, el ejemplo que acabo de dar es el caso m√°s extremo, pero solo puedo cortarlo tanta holgura. Santa historia corta: la pel√≠cula simplemente no funciona porque el gui√≥n no puede capitalizar las habilidades de Pinchot y Sartain para imponerse a Stan y Ollie . En su lugar, el gui√≥n capitaliza la exposici√≥n de la trama y los chistes cojos. Ver a esta pel√≠cula est√° observando b√°sicamente dos excelentes impersonadores que no recibieron material real para trabajar con. No una buena pel√≠cula, sino una incre√≠ble ayuda para dormir. Digo que le d√© una se√±orita y se pegue con el verdadero negocio (siempre y cuando gu√≠e Claro de \"atoll k\" y \"ser grande\").', 'Ignora el comentario ante el m√≠o. El espect√°culo es el genio c√≥mico, la mayor√≠a de los showes subestimados. 3 razones por las que. Raz√≥n 1, los flashbacks son hilarantes, a los que se preocupa si no tienen absolutamente nada que ver con la historia, me hacen re√≠r tan dif√≠cil que duele y no es la idea de un buen programa de comedia. Las personas que piensan que esto no es gracioso, simplemente no tiene sentido del humor. A quien le importa si son al azar, es un T.V. Mostrar, puedes hacer lo que quieras con √©l y la gente dice que no es realista, tampoco es una caricatura, est√° animada. Raz√≥n 2, los chistes son generalmente hilarantes, Seth McFarlane y los otros escritores son tan divertidos, todo, desde las referencias de Star Wars a burlarse de las celebridades, los amo a todos. Y la raz√≥n 3, las l√≠neas de la historia son simplemente brillantes y divertidas, establecieron todos los chistes con su naturaleza loca y el roseo, todo lo m√°s divertido. Me atrap√≥ esta noche en la BBC2 y nunca he vuelto, tengo todo 7 Seasons, adem√°s del tipo de cine (Stewie Griffin: la historia no contada), tengo varias camisetas, carteles y mi colecci√≥n todav√≠a est√° creciendo. Nunca he recibido una colecci√≥n de ning√∫n programa de televisi√≥n antes, pero esto fue tan gracioso que solo ten√≠a que obtener todo. Simplemente mira la temporada 3 y conf√≠a en m√≠, estar√°s enganchado de la vida, el espect√°culo m√°s divertido jam√°s creado y espero que nunca termine!', 'Esta es una excelente comedia rom√°ntica fuera de la pared sobre el amor, el trabajo, lo que brota al gusto p√∫blico y las crisis de la mediana edad.El personaje principal es un talentoso director de cine que decide hacer una pel√≠cula tonta PG-13 para salir de Hock con el IRS.Tiene un excelente reparto, una amplia gama de humor (desde un l√≠mite hasta slapstick), y bien escritura.Tambi√©n es un asombroso env√≠o de la industria cinematogr√°fica.El metacommentario incluye varios recortes excelentes entre la realidad y la pel√≠cula que se est√° haciendo, y en algunos lugares, la pel√≠cula sale del estricto realismo.El resultado es una obra maestra multidimensional de humor de mediana edad.', 'Muchas de las pel√≠culas cl√°sicas de los √∫ltimos 60 no han conservado su capacidad para molestar y enfrentar a la audiencia. \"En sangre fr√≠a\" no ha perdido una onza de su poder. Es excepcionalmente bien hecho pero obliga al espectador a pensar. Algunos se han quejado no solo de la pel√≠cula, sino tambi√©n sobre la fuente de Truman Capote \"Novela de no ficci√≥n\", que el mensaje central no es sujeto. Eso puede ser cierto, pero esto es definitivamente un caso en el que la falta de ambig√ºedad no restaba de la pel√≠cula en absoluto. Es refrescante, especialmente considerando los dramas morales simplistas y manipuladores de hoy, para ver una pel√≠cula con una voz pol√≠tica convencida que no tiene miedo de obligar a la audiencia a considerar su punto de vista. Para ser honesto, no estoy seguro de si estoy de acuerdo con el mensaje central de la pel√≠cula, pero admiro su audacia, sin embargo. Incluso si no est√° de acuerdo con el mensaje de la pena anti-capital, hay mucho que admirar sobre la pel√≠cula. La actuaci√≥n de los dos clientes potenciales es estupenda. Scott Wilson (todav√≠a uno de los actores m√°s subestimados de la historia) se est√° enfriando como el l√≠der nihil√≠stico, uno que usa su carisma para ocultar sus debilidades. Robert Blake tambi√©n se est√° enfriando como m√°s sumisos de los dos y la de la conciencia. Su car√°cter obviamente tiene una voz de la raz√≥n, pero est√° aterrorizada a ir contra Wilson (hay una buena cantidad de subtexto homoer√≥tico en la parte de su personaje). La cinematograf√≠a es excelente, elegante pero arenosa y realmente dando la impresi√≥n, el espectador est√° observando un documental. Agregue otra puntuaci√≥n cl√°sica de Quincey Jones, y tiene una obra maestra. (9/10)']\n",
      "Head of the training set y: \n",
      "[0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print head of the training set x and y\n",
    "print(\"Head of the training set x: \")\n",
    "print(train_x[:5])\n",
    "print(\"Head of the training set y: \")\n",
    "print(train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From train sets, remove 20% and save as test sets\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = train_x[int(len(train_x) * 0.2):]\n",
    "test_y = train_y[int(len(train_y) * 0.2):]\n",
    "train_x = train_x[:int(len(train_x) * 0.2)]\n",
    "train_y = train_y[:int(len(train_y) * 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_x: 10776\n",
      "Length of train_y: 10776\n",
      "Length of test_x: 43106\n",
      "Length of test_y: 43106\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Print length of all sets train and test for X and Y\n",
    "print(\"Length of train_x: \" + str(len(train_x)))\n",
    "print(\"Length of train_y: \" + str(len(train_y)))\n",
    "print(\"Length of test_x: \" + str(len(test_x)))\n",
    "print(\"Length of test_y: \" + str(len(test_y)))\n",
    "\n",
    "print(type(train_x))\n",
    "print(type(train_y))\n",
    "print(type(test_x))\n",
    "print(type(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 String cleaning, tokenization and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the spanish stopwords, I will be importing it from the internet and merge it with the spanish stopwords from the nltk library.\n",
    "Link: https://github.com/Alir3z4/stop-words/blob/master/spanish.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_es = stopwords.words('spanish')\n",
    "\n",
    "# Import stopwords from txt\n",
    "with open(folder + 'stopwords.txt', 'r') as f:\n",
    "    stopwords_es.extend(f.read().splitlines())\n",
    "\n",
    "# To lower, remove duplicates\n",
    "stopwords_es = list(set(stopwords_es))\n",
    "stopwords_es = [word.lower() for word in stopwords_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def clean_string(tweet):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        - tweet string\n",
    "    Output:\n",
    "        - tokenized, cleaned, and stemmed tweet\n",
    "    \"\"\"\n",
    "    # Cleaning the text\n",
    "\n",
    "    # To lower\n",
    "    tweet = tweet.lower()\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    # Remove usernames\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    # Remove hashtag symbol only\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # Remove special characters\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d', '', tweet)\n",
    "\n",
    "    # Tokenize\n",
    "    tweet = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tweet = [word for word in tweet if word not in stopwords_es]\n",
    "\n",
    "    # Stemming\n",
    "    tweet = [stemmer.stem(word) for word in tweet]\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tweets function\n",
    "def count_words(result, texts, ys):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - result: dictionary containing a pair (word, label) and its frequency\n",
    "        - texts: a list of texts\n",
    "        - ys: a list of labels\n",
    "    Output:\n",
    "        - result: dictionary containing a pair (word, label) and its frequency\n",
    "    \"\"\"\n",
    "    for y, text in tqdm(zip(ys, texts), total=len(texts)):\n",
    "        tokens = clean_string(text)\n",
    "        for word in tokens:\n",
    "            pair = (word, y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 214.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('amo', 1): 1,\n",
       " ('gat', 1): 1,\n",
       " ('gust', 1): 1,\n",
       " ('cin', 1): 1,\n",
       " ('gust', 0): 1,\n",
       " ('marisc', 0): 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "tweets = [\"Amo a mi gato\", \"Me gusta el cine\", \"No me gusta el marisco\"]\n",
    "ys = [1, 1, 0]\n",
    "count_words(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10776/10776 [01:14<00:00, 144.26it/s]\n"
     ]
    }
   ],
   "source": [
    "freqs = count_words({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        word: the word to look up\n",
    "        label: the label corresponding to the word\n",
    "    Output:\n",
    "        n: the number of times the word with its corresponding label appears.\n",
    "    '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += 1\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += 1\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents (*hint: use sum(<np_array>))\n",
    "    D_pos = np.sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)\n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs, word, 1)\n",
    "        freq_neg = lookup(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004083153408609874\n",
      "71824\n"
     ]
    }
   ],
   "source": [
    "# If naive_results.pkl file exists inport, else create\n",
    "if os.path.exists(folder + 'naive_results.pkl'):\n",
    "    with open(folder + 'naive_results.pkl', 'rb') as f:\n",
    "        naive_results = pickle.load(f)\n",
    "else:\n",
    "    naive_results = {}\n",
    "    naive_results['logprior'], naive_results['loglikelihood'] = train_naive_bayes(freqs, train_x, train_y)\n",
    "    with open(folder + 'naive_results.pkl', 'wb') as f:\n",
    "        pickle.dump(naive_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = clean_string(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []\n",
    "    for tweet in tqdm(test_x):\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43106/43106 [04:54<00:00, 146.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is much better than the previous notebook which had 0.6 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze my tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Import tweets.pkl\n",
    "with open('tweets.pkl', 'rb') as f:\n",
    "    tweets = pickle.load(f)\n",
    "tweets = [tweet.full_text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3359628452756205\n",
      "-0.03643020971222288\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "text = \"Que ricos estaban los cereales\"\n",
    "print(naive_bayes_predict(text, logprior, loglikelihood))\n",
    "text = \"Nop me ha gustado anda la comida de hoy\"\n",
    "print(naive_bayes_predict(text, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef82f28d7a874cc88a9fffec5f2cce6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "# tweets_df. column text = tweets, prediction = \"\"\n",
    "tweets_df = pd.DataFrame(tweets, columns=['text'])\n",
    "tweets_df['prediction'] = tweets_df['text'].swifter.apply(lambda tweet: naive_bayes_predict(tweet, logprior, loglikelihood))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  prediction\n",
      "806                             @aucsi ansioso espero üòé   -0.161808\n",
      "76                             @cyberlentejas Kaczynski    0.004083\n",
      "1642                           qe bonita la luna amigos    1.422869\n",
      "1719  4 masibon qe me meto en el dia gracias ansieda...    3.344203\n",
      "659   1h de jornada laboral y ya estoy hasta los huevos    0.972390\n"
     ]
    }
   ],
   "source": [
    "# Print 5 random tweets with the predicted class\n",
    "print(tweets_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the predictions in a list\n",
    "preds_list = tweets_df['prediction'].tolist()\n",
    "\n",
    "mean = np.mean(preds_list)\n",
    "plt.hist(preds_list, bins=20)\n",
    "plt.axvline(mean, color='r')\n",
    "plt.legend(['Mean = %0.4f' % mean])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pros de terminar examenes: terminas examenes\n",
      "cons: tener qe recoger la cuadra de cuarto en la qe he estao viviendo sin cuidao alguno qe de vd parece q han vivio aqi 2 yeguas\n",
      "7.01601365032189\n",
      "\n",
      "otra vez mi familia diciendo qe ense√±e los tatus y las cosas como si fuera un escaparate delante to la familia y qe qe son qe por qe me los hago mira me tenio qe levantar e irme qe ansiedad qe incomodidad\n",
      "9.146069474044628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if any prediction is above 5 sigma or below -5 sigma\n",
    "sigma = np.std(preds_list)\n",
    "important = np.where((preds_list > 5*sigma) | (preds_list < -5*sigma))\n",
    "\n",
    "# Print the important tweets\n",
    "for i in important[0]:\n",
    "    print(tweets_df.iloc[i]['text'])\n",
    "    print(tweets_df.iloc[i]['prediction'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both tweets are somewhat negative. This may be because my tweets are written with abbreviations and my tweets may have not been well stemmed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84be8c82a7a3e86db451810d88cd8e67624a6eca0a3f1ff4d59d995ee7a917f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
